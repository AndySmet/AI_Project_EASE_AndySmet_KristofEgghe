{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hyperparameters.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization\n",
        "We load the 5 random train test splits and for each train test split we optimize the hyper parameters and choose the ones with the highest recall. Then we calculate the mean recall an ndcg and the standard deviations between the 5 train test splits and look at the optimal hyperparameters for each spli."
      ],
      "metadata": {
        "id": "Z0pvvZXItrT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set to the correct paths\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "allData=False\n",
        "\n",
        "\n",
        "path=\"/home/kristof/Desktop/AI_Project/\"\n",
        "\n",
        "\n",
        "# path=\"./drive/MyDrive/AI Project/\"\n",
        "\n",
        "\n",
        "interactions_csv_path = path+\"datasets/TestDataPreprocessed_interactions.csv\"\n",
        "# goodreads_path = '/home/andy/Desktop/AI_Project/datasets/Goodreads/'\n",
        "folds_path=path+\"datasets/\"\n",
        "consgroup_path=path+\"datasets/TestDataConsGroupdict.pickle\"\n",
        "reversegroup_path=path+\"datasets/TestDataReverseGroupdict.pickle\"\n",
        "groupdict_path=path+\"datasets/TestDataGroupdict.pickle\"\n",
        "userdic_path=path+\"datasets/TestDataUser_dic.pickle\"\n",
        "books_df_g_path=path+\"datasets/books_df_grouped.csv\"\n"
      ],
      "metadata": {
        "id": "gp2pMiRGQZBz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vqRtR_OMQf-K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EASER modified to work with groups\n",
        "Easer normally sets the self similarity to zero to avoid items recommending themselves, but when all volumes are combined into one group we do want those groups to be similar to themselves so we set the diagonal to zeros except for the possitions of the grouped volumes\n",
        "\n",
        "\n",
        "Calculating the inverse of the dense item to item matrix is very expensive memory wise. To combat this we use a sparse approximation https://arxiv.org/pdf/1904.13033.pdf#page=6"
      ],
      "metadata": {
        "id": "zrLi6kLx0nas"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3AoId8QsOFzK"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Embarrassingly Shallow Autoencoder\n",
        "from scipy.sparse import find\n",
        "import itertools\n",
        "import re\n",
        "import numpy as np\n",
        "class Ease():\n",
        "    def __init__(self, K=10, l=1000):\n",
        "        #how many predictions to make\n",
        "        self.K = K\n",
        "        #lambda L2 regularization term\n",
        "\n",
        "        self.l = l\n",
        "        #weight matrix B\n",
        "        self.B = None\n",
        "        #threshold for corrcoeff matrix in fit\n",
        "        self.threshold = 0.65\n",
        "        #new diag that helps for adjusted self-similarity scores\n",
        "        self.newDiag = []\n",
        "\n",
        "    def fit(self, X):\n",
        "        #implementation of sparse inverse calculations\n",
        "        #get ItemxItem gram matirx G\n",
        "        G = (X.T @ X).toarray().astype(np.float32)\n",
        "        #calculate corrcoef matrix on G\n",
        "        c=np.corrcoef(G).astype(np.float32)\n",
        "        c = np.nan_to_num(c)\n",
        "        \n",
        "        #set threshold for maximal size of blocks\n",
        "        maxi=1000\n",
        "        # threshold=0.3 -> 850 items worden gerecommend\n",
        "        # maxi=1500\n",
        "        #first set all items in the corrcoef matrix that are above a threshold to 1\n",
        "        for row in c.T:\n",
        "          l=list(reversed(np.argsort(row)))[:maxi]\n",
        "          for i in l:\n",
        "            if row[i]>self.threshold:\n",
        "              row[i]=1\n",
        "            else:\n",
        "              break\n",
        "        #binarize corrcoef matrix\n",
        "        A=np.where(c==1, 1, 0)\n",
        "        \n",
        "\n",
        "        #get list of indices in descending order of column sums\n",
        "        res=A.sum(axis=0)\n",
        "        \n",
        "        l=np.array(list(reversed(np.argsort(res))))\n",
        "        setje=set()\n",
        "        #iterate over indices and generate submatrices\n",
        "        spars=scipy.sparse.lil_matrix(G.shape)\n",
        "        for i in l:\n",
        "          if i in setje:\n",
        "              continue\n",
        "          #get itemindexes where element == 0\n",
        "          col=A[:,i]\n",
        "          itemindexes=np.where(col==1)\n",
        "          #construct submatrix\n",
        "          sub=G[np.ix_(itemindexes[0],itemindexes[0])]\n",
        "          diag=scipy.sparse.dia_matrix(sub.shape)\n",
        "          #set submatrixs diagonal to lambda + submatrix\n",
        "          diag.setdiag([self.l]*sub.shape[0])\n",
        "          sub2=(sub+diag)\n",
        "          #calculate inverse and set diagonal to 0 (self-similarity)\n",
        "          P=np.linalg.inv(sub2)\n",
        "          B=-P/P.diagonal()\n",
        "         \n",
        "          \n",
        "          np.fill_diagonal(B,0)\n",
        "          # store values in sparse matrix\n",
        "          for i in itertools.product(range(B.shape[0]),range(B.shape[0])):\n",
        "            i1=i[0]\n",
        "            j1=i[1]\n",
        "            spars[itemindexes[0][i1],itemindexes[0][j1]]=B[i1,j1]\n",
        "          for j in itemindexes[0]:\n",
        "              setje.add(j)\n",
        "        # d = [0 if len(reverseGroupdic[key])==1 else similarityScoreDic[key] for key in similarityScoreDic]\n",
        "        spars.setdiag(self.newDiag)\n",
        "        self.B = spars.toarray()\n",
        "    def predict(self, X, firstUserID):\n",
        "        \n",
        "        S = X*self.B\n",
        "        users = set(X.nonzero()[0])\n",
        "        \n",
        "      \n",
        "        U, I, V = [], [], []\n",
        "        real_groups=list(cons_group_dic.keys())\n",
        "\n",
        "        for user in users:\n",
        "            history_of_user=randomSplit_df.loc[randomSplit_df['user_id']==user+firstUserID].history.item()\n",
        "            U.extend([user] * self.K)\n",
        "            \n",
        "\n",
        "            # a=np.array([S[user][i] if i not in history_of_user or (len(reverseGroupdic[i]) > 1 and len(re.split(' Volume| Vol\\.| vol\\.| #', books_df_g.loc[books_df_g['group_id'] == list(cons_group_dic.keys())[i]].title.item())) > 1) else -self.l*2 for i in range(len(S[user])) ])\n",
        "            a = []\n",
        "            for i in range(len(S[user])):\n",
        "             \n",
        "              try:\n",
        "                lastViewedIndex = (len(history_of_user) - 1 - history_of_user[::-1].index(i))\n",
        "              except:\n",
        "                lastViewedIndex = len(history_of_user)\n",
        "              if i not in history_of_user or (len(reverseGroupdic[real_groups[i]]) > 1 and len(re.split(' Volume| Vol\\.| vol\\.| #', books_df_g.loc[books_df_g['group_id'] == real_groups[i]].title.item())) > 1):\n",
        "                a.append(S[user][i]*((lastViewedIndex/len(history_of_user))**2)) \n",
        "              else:\n",
        "                a.append(-self.l*2)\n",
        "            # a = np.array([S[user][i]*history_of_user.count(i) if (len(reverseGroupdic[i]) > 1 and history_of_user.count(i)<len(reverseGroupdic[i])) else a[i] for i in range(len(a))])\n",
        "            items = np.array(a).argsort()[-self.K:][::-1]\n",
        "            I.extend(items)\n",
        "            V.extend(S[user][items])\n",
        "\n",
        "        score_matrix = scipy.sparse.csr_matrix((V, (U, I)), shape=X.shape).astype(np.float32)\n",
        "        return score_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization For each split"
      ],
      "metadata": {
        "id": "mipljlrL1hyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import vstack\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn.metrics\n",
        "import statistics\n",
        "import pickle\n",
        "import pandas as pd\n",
        "with open(consgroup_path, 'rb') as f:\n",
        "    cons_group_dic= pickle.load(f)\n",
        "with open(groupdict_path, 'rb') as f:\n",
        "    groupdic= pickle.load(f)\n",
        "with open(reversegroup_path, 'rb') as f:\n",
        "    reverseGroupdic= pickle.load(f)\n",
        "with open(userdic_path, 'rb') as f:\n",
        "    userdic= pickle.load(f)\n",
        "randomSplit_df=0\n",
        "books_df_g=pd.read_csv(books_df_g_path)\n",
        "books_df_g[\"group_id\"]=range(0,len(books_df_g))\n",
        "\n",
        "def recall_easer_hyperparameter_optimization(model,K):\n",
        "  global randomSplit_df\n",
        "\n",
        "  ease = model\n",
        "  recalls = []\n",
        "  ndcgs = []\n",
        "  y_pred_totals = []\n",
        "  hyperParams=[]\n",
        "  for s in range(1,6):\n",
        "    if allData:\n",
        "      randomSplit_df = pd.read_csv(folds_path+\"AllDatafold\"+str(s)+\".csv\")\n",
        "    else:\n",
        "      randomSplit_df = pd.read_csv(folds_path+\"TestDatafold\"+str(s)+\".csv\")\n",
        "    randomSplit_df['user_id'] = randomSplit_df.user_id.apply(lambda x:int(userdic[int(x)]))\n",
        "\n",
        "    randomSplit_df['history'] = randomSplit_df.history.apply(lambda x:[int(cons_group_dic[groupdic[int(y)]]) if y else y for y in x[1:-1].split(',')])\n",
        "    randomSplit_df['future'] = randomSplit_df.future.apply(lambda x:[int(cons_group_dic[groupdic[int(y)]]) if y else y for y in x[1:-1].split(',')])\n",
        "    shape = (randomSplit_df['user_id'].max() +1, len(cons_group_dic))\n",
        "\n",
        "    ease.newDiag = get_newDiag(randomSplit_df,shape)\n",
        "    train_x = create_sparse_matrix(randomSplit_df, column='history', shape=shape)\n",
        "    y_true = create_sparse_matrix(randomSplit_df, column='future', shape=shape)\n",
        "   \n",
        "    \n",
        "    recalls_fold = []\n",
        "    ndcgs_fold=[]\n",
        "    hyperParams_fold=[]\n",
        "    for lambda_ in [250,5000,50000]:\n",
        "      for threshold in [0.8,0.9]:\n",
        "        ease.threshold = threshold\n",
        "        ease.l=lambda_\n",
        "        ease.fit(train_x)\n",
        "\n",
        "        interval=int(train_x.shape[0]/20)\n",
        "        true_ndcg=0\n",
        "        true_recall=0\n",
        "        y_pred_total = scipy.sparse.csr_matrix((0,train_x.shape[1])).astype(np.float32)\n",
        "        \n",
        "        for j in range(20-1):\n",
        "          predict_split=train_x[np.ix_(list(range(interval*j,interval*(j+1))),list(range(train_x.shape[1])))]\n",
        "          true_split=y_true[np.ix_(list(range(interval*j,interval*(j+1))),list(range(train_x.shape[1])))]\n",
        "          y_pred = ease.predict(predict_split, interval*j)\n",
        "          y_pred_total = vstack((y_pred_total, y_pred))\n",
        "          scores = scipy.sparse.lil_matrix(y_pred.shape)\n",
        "          scores[y_pred.multiply(true_split).astype(bool)] = 1\n",
        "          scores = scores.tocsr()\n",
        "          scores = sparse_divide_nonzero(scores, scipy.sparse.csr_matrix(true_split.sum(axis=1))).sum(axis=1)\n",
        "          true_recall+=scores.mean()*(interval/train_x.shape[0])\n",
        "      \n",
        "          ndcg = sklearn.metrics.ndcg_score(true_split.toarray(), y_pred.toarray(), k=K)\n",
        "          true_ndcg+=ndcg*(interval/train_x.shape[0])\n",
        "\n",
        "        predict_split=train_x[np.ix_(list(range(interval*19,train_x.shape[0])),list(range(train_x.shape[1])))]\n",
        "        true_split=y_true[np.ix_(list(range(interval*19,train_x.shape[0])),list(range(train_x.shape[1])))]\n",
        "        y_pred = ease.predict(predict_split, interval*19)\n",
        "        y_pred_total = vstack((y_pred_total, y_pred))\n",
        "        scores = scipy.sparse.lil_matrix(y_pred.shape)\n",
        "        scores[y_pred.multiply(true_split).astype(bool)] = 1\n",
        "        scores = scores.tocsr()\n",
        "        scores = sparse_divide_nonzero(scores, scipy.sparse.csr_matrix(true_split.sum(axis=1))).sum(axis=1)\n",
        "        true_recall+=scores.mean()*(interval/train_x.shape[0])\n",
        "\n",
        "        \n",
        "        ndcg = sklearn.metrics.ndcg_score(true_split.toarray(), y_pred.toarray(), k=K)\n",
        "        true_ndcg+=ndcg*(interval/train_x.shape[0])\n",
        "        print(\"recall @ {}: {:.4f}, ndcg: {:.4f} params lambda:{}, threshold={}, seed= {}\".format(K,true_recall,true_ndcg ,lambda_, threshold,s))\n",
        "\n",
        "        \n",
        "        ndcgs_fold.append(true_ndcg)\n",
        "        recalls_fold.append(true_recall)\n",
        "        hyperParams_fold.append((lambda_, threshold))\n",
        "    max_value = max(recalls_fold)\n",
        "    max_index = recalls_fold.index(max_value)\n",
        "    recalls.append(recalls_fold[max_index])\n",
        "    ndcgs.append(ndcgs_fold[max_index])\n",
        "    hyperParams.append(hyperParams_fold[max_index])\n",
        "  \n",
        "  meanRecall = statistics.mean(recalls)\n",
        "  stdevRecall = statistics.stdev(recalls)\n",
        "\n",
        "  meanNdcg = statistics.mean(ndcgs)\n",
        "  stdevNdcg = statistics.stdev(ndcgs)\n",
        "  print(\"mean recall @ {}: {:.4f}, stdev: {:.4f}\".format(K, meanRecall, stdevRecall))\n",
        "  print(\"mean ndcg @ {}: {:.4f}, stdev: {:.4f}\".format(K, meanNdcg, stdevNdcg))\n",
        "  print(\"used hyperParameters (lambda, threshold): {}\".format(hyperParams))\n",
        "  return meanNdcg,meanRecall, hyperParams\n",
        "  "
      ],
      "metadata": {
        "id": "vhh18MLzPqyD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "\n",
        "#Create scipy csr matrix\n",
        "def create_sparse_matrix(sessions_df, column='history', shape=None):\n",
        "  #flatten\n",
        "  user_ids = []\n",
        "  item_ids = []\n",
        "  for idx, row in sessions_df.iterrows():\n",
        "    items = row[column]\n",
        "    user = row['user_id']\n",
        "    user_ids.extend([user] * len(items))\n",
        "    item_ids.extend(items)\n",
        "  #create csr matrix\n",
        "  values = np.ones(len(user_ids))\n",
        "\n",
        "  matrix = scipy.sparse.csr_matrix((values, (user_ids, item_ids)), shape=shape, dtype=np.int32)\n",
        "  return matrix\n",
        "\n",
        "\n",
        "def get_newDiag(df,shape):\n",
        "  self_similarity_n={}\n",
        "  self_similarity={}\n",
        "  for idx,row in df.iterrows():\n",
        "    l=row['history']\n",
        "    histset=set(l)\n",
        "    real_group_id_list=list(cons_group_dic.keys())\n",
        "    for concgroup in histset:\n",
        "      # print(concgroup,histset)\n",
        "      if not concgroup:\n",
        "        continue\n",
        "      real_group_id=real_group_id_list[concgroup]\n",
        "      grouplist=reverseGroupdic[real_group_id]\n",
        "      if len(grouplist) ==1:\n",
        "        self_similarity[concgroup]=0\n",
        "        # self_similarity_n[concgroup]=1\n",
        "\n",
        "      else:\n",
        "        self_similarity[concgroup]=1\n",
        "  # for i in self_similarity:\n",
        "  #   self_similarity[i]=self_similarity[i]/(self_similarity_n[i]+20)\n",
        "  new_diag=[self_similarity[i] if i in self_similarity else 0 for i in list(range(shape[1]))]\n",
        "  return new_diag"
      ],
      "metadata": {
        "id": "DUuQtmxtTmjB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_divide_nonzero(a: scipy.sparse.csr_matrix, b: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix:\n",
        "    return a.multiply(sparse_inverse_nonzero(b))\n",
        "\n",
        "def sparse_inverse_nonzero(a: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix:\n",
        "    inv_a = a.copy()\n",
        "    inv_a.data = 1 / inv_a.data\n",
        "    return inv_a\n",
        "\n",
        "\n",
        "K = 10\n",
        "for K in [10]:\n",
        "  \n",
        "  ease = Ease(K=K)\n",
        "  ndcg,recall, y_pred =recall_easer_hyperparameter_optimization(ease,K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAOL_giPlNj",
        "outputId": "ed64eb8b-c0de-4900-e84d-84c7fdd0e84d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall @ 10: 0.1899, ndcg: 0.4558 params lambda:250, threshold=0.8, seed= 1\n",
            "recall @ 10: 0.2012, ndcg: 0.4665 params lambda:250, threshold=0.9, seed= 1\n",
            "recall @ 10: 0.1966, ndcg: 0.4673 params lambda:5000, threshold=0.8, seed= 1\n",
            "recall @ 10: 0.2192, ndcg: 0.4887 params lambda:5000, threshold=0.9, seed= 1\n",
            "recall @ 10: 0.2238, ndcg: 0.4965 params lambda:50000, threshold=0.8, seed= 1\n",
            "recall @ 10: 0.2449, ndcg: 0.5129 params lambda:50000, threshold=0.9, seed= 1\n",
            "recall @ 10: 0.1830, ndcg: 0.4449 params lambda:250, threshold=0.8, seed= 2\n",
            "recall @ 10: 0.1920, ndcg: 0.4519 params lambda:250, threshold=0.9, seed= 2\n",
            "recall @ 10: 0.1859, ndcg: 0.4515 params lambda:5000, threshold=0.8, seed= 2\n",
            "recall @ 10: 0.2035, ndcg: 0.4686 params lambda:5000, threshold=0.9, seed= 2\n",
            "recall @ 10: 0.2060, ndcg: 0.4754 params lambda:50000, threshold=0.8, seed= 2\n",
            "recall @ 10: 0.2264, ndcg: 0.4903 params lambda:50000, threshold=0.9, seed= 2\n",
            "recall @ 10: 0.2169, ndcg: 0.4784 params lambda:250, threshold=0.8, seed= 3\n",
            "recall @ 10: 0.2306, ndcg: 0.4903 params lambda:250, threshold=0.9, seed= 3\n",
            "recall @ 10: 0.2256, ndcg: 0.4904 params lambda:5000, threshold=0.8, seed= 3\n",
            "recall @ 10: 0.2490, ndcg: 0.5105 params lambda:5000, threshold=0.9, seed= 3\n",
            "recall @ 10: 0.2539, ndcg: 0.5183 params lambda:50000, threshold=0.8, seed= 3\n",
            "recall @ 10: 0.2745, ndcg: 0.5320 params lambda:50000, threshold=0.9, seed= 3\n",
            "recall @ 10: 0.1936, ndcg: 0.4698 params lambda:250, threshold=0.8, seed= 4\n",
            "recall @ 10: 0.2049, ndcg: 0.4802 params lambda:250, threshold=0.9, seed= 4\n",
            "recall @ 10: 0.2085, ndcg: 0.4865 params lambda:5000, threshold=0.8, seed= 4\n",
            "recall @ 10: 0.2304, ndcg: 0.5082 params lambda:5000, threshold=0.9, seed= 4\n",
            "recall @ 10: 0.2425, ndcg: 0.5196 params lambda:50000, threshold=0.8, seed= 4\n",
            "recall @ 10: 0.2651, ndcg: 0.5366 params lambda:50000, threshold=0.9, seed= 4\n",
            "recall @ 10: 0.2244, ndcg: 0.4869 params lambda:250, threshold=0.8, seed= 5\n",
            "recall @ 10: 0.2379, ndcg: 0.4975 params lambda:250, threshold=0.9, seed= 5\n",
            "recall @ 10: 0.2278, ndcg: 0.4939 params lambda:5000, threshold=0.8, seed= 5\n",
            "recall @ 10: 0.2523, ndcg: 0.5154 params lambda:5000, threshold=0.9, seed= 5\n",
            "recall @ 10: 0.2498, ndcg: 0.5169 params lambda:50000, threshold=0.8, seed= 5\n",
            "recall @ 10: 0.2702, ndcg: 0.5298 params lambda:50000, threshold=0.9, seed= 5\n",
            "mean recall @ 10: 0.2562, stdev: 0.0202\n",
            "mean ndcg @ 10: 0.5203, stdev: 0.0191\n",
            "used hyperParameters (lambda, threshold): [(50000, 0.9), (50000, 0.9), (50000, 0.9), (50000, 0.9), (50000, 0.9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S24eVKJphNQs"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}